{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13816095,"sourceType":"datasetVersion","datasetId":8797965},{"sourceId":13820876,"sourceType":"datasetVersion","datasetId":8801591}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 0 ‚Äî Dataset Validation\n\nBefore preprocessing, we validated the structure of both datasets used for SEN12MS. \n\nThe Sentinel-2 RGB+NIR tiles (from `deepnir-nir-rgb-sen12ms-dataset`) were confirmed to follow the naming pattern `ROIs<ROI>_<season>_s<scene>_p<patch>.png` and are organized into train/val/test splits with RGB in `_A` folders and NIR in `_B` folders. \n\nThe LC masks uploaded from the official SEN12MS dataset were confirmed to be TIFF files stored inside nested `lc_<sceneID>` folders, following the pattern `ROIs<ROI>_<season>_lc_<sceneID>_p<PATCHID>.tif`. \n\nThese patterns match perfectly and allow deterministic mapping between S2 image tiles and LC masks for segmentation.\n","metadata":{}},{"cell_type":"code","source":"import os\nroot = \"/kaggle/input/deepnir-nir-rgb-sen12ms-dataset\"\nprint(\"Dataset accessible:\", os.path.exists(root))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo \"=== S2 ROOT ===\"\n!ls -d /kaggle/input/deepnir-nir-rgb-sen12ms-dataset/sen12ms_All_seasons/*\n\n!echo \"\"\n!echo \"=== Example files from train_A ===\"\n!ls /kaggle/input/deepnir-nir-rgb-sen12ms-dataset/sen12ms_All_seasons/train_A | head\n\n!echo \"\"\n!echo \"=== Example files from train_B ===\"\n!ls /kaggle/input/deepnir-nir-rgb-sen12ms-dataset/sen12ms_All_seasons/train_B | head","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root1 = \"/kaggle/input/sen12ms-lc\"\nprint(\"Dataset accessible:\", os.path.exists(root1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo \"\"\n!echo \"=== LC spring folder ===\"\n!ls -d /kaggle/input/sen12ms-lc/ROIs1158_spring_lc/*\n\n!echo \"\"\n!echo \"=== Example LC files (spring) ===\"\n!ls /kaggle/input/sen12ms-lc/ROIs1158_spring_lc/ROIs1158_spring | head","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo \"=== Inside lc_1 ===\"\n!ls /kaggle/input/sen12ms-lc/ROIs1158_spring_lc/ROIs1158_spring/lc_1 | head\n\n!echo \"\"\n!echo \"=== Inside lc_100 ===\"\n!ls /kaggle/input/sen12ms-lc/ROIs1158_spring_lc/ROIs1158_spring/lc_100 | head\n\n!echo \"\"\n!echo \"=== Try to find any mask file in the spring LC folder ===\"\n!find /kaggle/input/sen12ms-lc/ROIs1158_spring_lc/ROIs1158_spring -type f | head\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 1 ‚Äî Create Processing Directory\nA dedicated directory `sen12ms_processed` was created inside the Kaggle working environment. All generated outputs (bands, masks, metadata) will be stored here for easy tracking and export.","metadata":{}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/sen12ms_processed\n!echo \"Created /kaggle/working/sen12ms_processed\"\n!mkdir -p /kaggle/working/indexes\n!echo \"Created /kaggle/working/indexes\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2 ‚Äî Full Dataset Indexing in Safe Batches\nTo avoid performance issues when scanning more than 40,000 SEN12MS tiles, we processed the dataset in controlled batches of 5,000 files per iteration. Each batch extracted the ROI, season, scene ID, and patch ID from the S2 filename, resolved the corresponding LC path, and wrote a partial CSV file. These partial index files will be merged into a final unified mapping table used for downstream preprocessing.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nS2_ROOT = \"/kaggle/input/deepnir-nir-rgb-sen12ms-dataset/sen12ms_All_seasons\"\nLC_ROOT = \"/kaggle/input/sen12ms-lc\"\n\ndef parse_s2_filename(filename):\n    base = filename.replace(\".png\", \"\")\n    parts = base.split(\"_\")\n    roi = parts[0]\n    season = parts[1]\n    scene = int(parts[2][1:])\n    patch = int(parts[3][1:])\n    return roi, season, scene, patch\n\ndef build_batch(split_name, start_idx, end_idx):\n    rgb_folder = os.path.join(S2_ROOT, f\"{split_name}_A\")\n    nir_folder = os.path.join(S2_ROOT, f\"{split_name}_B\")\n\n    rgb_files = sorted([f for f in os.listdir(rgb_folder) if f.endswith(\".png\")])\n    rgb_files = rgb_files[start_idx:end_idx]\n\n    rows = []\n\n    for f in rgb_files:\n        roi, season, scene, patch = parse_s2_filename(f)\n\n        rgb_path = os.path.join(rgb_folder, f)\n        nir_path = os.path.join(nir_folder, f)\n\n        lc_dir = f\"{roi}_{season}\"\n        lc_scene_dir = os.path.join(LC_ROOT, f\"{lc_dir}_lc\", lc_dir, f\"lc_{scene}\")\n        lc_filename = f\"{lc_dir}_lc_{scene}_p{patch}.tif\"\n        lc_path = os.path.join(lc_scene_dir, lc_filename)\n\n        if os.path.exists(lc_path):\n            rows.append({\n                \"rgb\": rgb_path,\n                \"nir\": nir_path,\n                \"lc\": lc_path,\n                \"roi\": roi,\n                \"season\": season,\n                \"scene\": scene,\n                \"patch\": patch\n            })\n\n    return pd.DataFrame(rows)\n\n# Process all splits\nsplits = [\"train\", \"val\", \"test\"]\nbatch_size = 10000\npartial_files = []\n\nfor split in splits:\n    print(f\"\\n=== Processing split: {split.upper()} ===\")\n\n    rgb_folder = os.path.join(S2_ROOT, f\"{split}_A\")\n    all_files = sorted([f for f in os.listdir(rgb_folder) if f.endswith(\".png\")])\n    total = len(all_files)\n\n    print(f\"{total} files found in {split}_A\")\n\n    for start in range(0, total, batch_size):\n        end = min(start + batch_size, total)\n        print(f\"  ‚Üí Batch {start} to {end}\")\n\n        df_batch = build_batch(split, start, end)\n\n        out_csv = f\"/kaggle/working/indexes/index_{split}_{start}_{end}.csv\"\n        df_batch.to_csv(out_csv, index=False)\n        partial_files.append(out_csv)\n\nprint(\"\\nBatched index build complete.\")\npartial_files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2.1 ‚Äî Merge Partial Index Files\nAfter building the SEN12MS S2‚ÜíLC mapping in safe batches, all partial index CSVs were merged into a single unified index file. This file lists the full paths for RGB and NIR Sentinel-2 tiles along with their matching LandCover masks and metadata fields (ROI, season, scene, patch). This consolidated index enables efficient and fully aligned preprocessing in later stages.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport glob\nimport os\n\n# Folder with partial CSVs\nINDEX_DIR = \"/kaggle/working/indexes\"\n\n# Find partial CSVs inside the folder\npartial_csvs = sorted(glob.glob(os.path.join(INDEX_DIR, \"index_*.csv\")))\n\nprint(\"Found\", len(partial_csvs), \"partial CSVs\")\n\ndfs = []\nfor f in partial_csvs:\n    print(\"Merging:\", f)\n    dfs.append(pd.read_csv(f))\n\ndf_full = pd.concat(dfs, ignore_index=True)\nprint(\"\\nFinal merged shape:\", df_full.shape)\n\n# Save final merged CSV OUTSIDE the folder, in /kaggle/working\ndf_full.to_csv(\"/kaggle/working/sen12ms_index.csv\", index=False)\n\nprint(\"\\nSaved final index as /kaggle/working/sen12ms_index.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3 ‚Äî 4-Band S2 Extraction (B2, B3, B4, B8)\nUsing the unified index, each Sentinel-2 sample was loaded in batches of 5000 images. The RGB PNG (containing B2, B3, B4) and NIR PNG (containing B8) were read, normalized to [0,1], and stacked into 4-band tensors of shape (64√ó64√ó4). LandCover TIFF files were loaded in parallel to provide the raw integer label mask. Each batch was saved to disk to avoid memory overload.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\nimport os\nimport tifffile\n\n# Ensure batches folder exists\nos.makedirs(\"/kaggle/working/sen12ms_batches\", exist_ok=True)\n\n# Load final unified index\ndf = pd.read_csv(\"/kaggle/working/sen12ms_index.csv\")\nprint(\"Total samples:\", df.shape[0])\n\nbatch_size = 10000\n\ndef load_rgb(path):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if img.shape[:2] != (64, 64):\n        img = cv2.resize(img, (64, 64), interpolation=cv2.INTER_NEAREST)\n\n    return img.astype(np.float32) / 255.0\n\ndef load_nir(path):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n\n    if img is None:\n        raise ValueError(\"NIR image missing: \" + path)\n\n    if img.ndim == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    if img.shape != (64, 64):\n        img = cv2.resize(img, (64, 64), interpolation=cv2.INTER_NEAREST)\n\n    img = img.astype(np.float32) / 255.0\n    return img.reshape(64, 64, 1)\n\nbatch_id = 0\n\nfor start in range(0, df.shape[0], batch_size):\n    end = min(start + batch_size, df.shape[0])\n    print(f\"\\n=== Processing BAND batch {batch_id}: {start} to {end} ===\")\n\n    rows = df.iloc[start:end]\n\n    bands_batch = []\n\n    for idx, row in tqdm(rows.iterrows(), total=len(rows)):\n        rgb = load_rgb(row['rgb'])\n        nir = load_nir(row['nir'])\n\n        band4 = np.concatenate([rgb, nir], axis=-1)   # (64,64,4)\n\n        bands_batch.append(band4)\n\n    bands_batch = np.array(bands_batch, dtype=np.float32)\n\n    # Save ONLY bands\n    np.savez_compressed(\n        f\"/kaggle/working/sen12ms_batches/sen12ms_bands_batch_{batch_id}.npz\",\n        bands=bands_batch\n    )\n\n    print(f\"Saved BAND batch {batch_id}\")\n    batch_id += 1\n\nprint(\"\\nAll BAND batches completed.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom skimage.filters import threshold_otsu\nfrom tqdm import tqdm\n\nBATCH_DIR = \"/kaggle/working/sen12ms_batches\"\n\ndef compute_ndwi(tile):\n    \"\"\"\n    NDWI = (Green - NIR) / (Green + NIR)\n    tile shape = (64, 64, 4)\n    channel 1 = G\n    channel 3 = NIR\n    \"\"\"\n    green = tile[:, :, 1].astype(\"float32\")\n    nir   = tile[:, :, 3].astype(\"float32\")\n    ndwi = (green - nir) / (green + nir + 1e-8)\n    return ndwi\n\n\n# ---- FIND ALL BAND BATCHES ----\nband_batches = sorted([f for f in os.listdir(BATCH_DIR) if f.startswith(\"sen12ms_bands_batch\")])\nprint(\"Found band batches:\", len(band_batches))\n\nfor batch_idx, bb in enumerate(band_batches):\n    print(f\"\\n=== Processing MASKS for batch {batch_idx}: {bb} ===\")\n\n    batch_path = os.path.join(BATCH_DIR, bb)\n    tiles = np.load(batch_path)[\"bands\"]    # (N, 64, 64, 4)\n\n    masks_out = []\n\n    for i in tqdm(range(tiles.shape[0])):\n        tile = tiles[i]\n\n        # 1) NDWI\n        ndwi = compute_ndwi(tile)\n\n        # 2) OTSU threshold\n        try:\n            thr = threshold_otsu(ndwi)\n        except:\n            thr = 0.0\n\n        # 3) Binary mask: water=1, land=0\n        mask = (ndwi > thr).astype(\"uint8\")\n        masks_out.append(mask)\n\n    masks_out = np.stack(masks_out, axis=0)\n\n    # ---- SAVE MASK BATCH ----\n    out_file = os.path.join(BATCH_DIR, f\"sen12ms_masks_batch_{batch_idx}.npz\")\n    np.savez_compressed(out_file, masks=masks_out)\n\n    print(\"Saved:\", out_file, masks_out.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 4 ‚Äî Merge All SEN12MS Batches Into Final Dataset\n\nAfter generating all sen12ms_bands_batch_*.npz and sen12ms_masks_batch_*.npz files, we now merge them into one unified dataset. This produces: sen12ms_bands.npy sen12ms_masks.npy sen12ms_metadata.json\n\nWe create one HDF5 file and write each batch into a section of the dataset. This avoids RAM overflows and avoids disk overflows.These outputs match the EuroSAT format and can be directly merged later.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport h5py\nfrom tqdm import tqdm\n\nBATCH_DIR = \"/kaggle/working/sen12ms_batches\"\nOUT_DIR = \"/kaggle/working/sen12ms_processed\"\n\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# -------------------------------------------------------\n# PASS 1 ‚Äî MERGE BAND BATCHES INTO sen12ms_bands.h5\n# -------------------------------------------------------\n\nbands_files = sorted([f for f in os.listdir(BATCH_DIR) if f.startswith(\"sen12ms_bands_batch\")])\nprint(\"Found\", len(bands_files), \"band batches\")\n\n# Count total samples\ntotal_samples = 0\nfor bf in bands_files:\n    arr = np.load(os.path.join(BATCH_DIR, bf))\n    total_samples += arr[\"bands\"].shape[0]\n\nprint(\"Total band samples:\", total_samples)\n\nbands_h5_path = os.path.join(OUT_DIR, \"sen12ms_bands.h5\")\n\nwith h5py.File(bands_h5_path, \"w\") as h5f:\n    ds = h5f.create_dataset(\"bands\",\n                            shape=(total_samples, 64, 64, 4),\n                            dtype=\"float32\")\n\n    write_ptr = 0\n    for bf in tqdm(bands_files):\n        b = np.load(os.path.join(BATCH_DIR, bf))[\"bands\"]\n        bs = b.shape[0]\n        ds[write_ptr:write_ptr+bs] = b\n        write_ptr += bs\n\nprint(\"Bands merged ‚Üí\", bands_h5_path)\n\n# -------------------------------------------------------\n# SAVE METADATA JSON\n# -------------------------------------------------------\n\nmetadata = {\n    \"total_samples\": total_samples,\n    \"image_size\": [64, 64],\n    \"bands\": [\"R\", \"G\", \"B\", \"NIR\"],\n    \"label_type\": \"LandCover\",\n}\n\nmeta_path = os.path.join(OUT_DIR, \"sen12ms_metadata.json\")\nwith open(meta_path, \"w\") as f:\n    json.dump(metadata, f, indent=4)\n\nprint(\"\\nMetadata saved ‚Üí\", meta_path)\n\nprint(\"\\n=== ALL MERGES COMPLETE ===\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport h5py\nfrom tqdm import tqdm\n\nBATCH_DIR = \"/kaggle/working/sen12ms_batches\"\nOUT_DIR = \"/kaggle/working/processed\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nmask_files = sorted([f for f in os.listdir(BATCH_DIR) if f.startswith(\"sen12ms_masks_batch\")])\nprint(\"Found mask batches:\", len(mask_files))\n\n# ---------- Step 1: Count total samples ----------\ntotal = 0\nfor mf in mask_files:\n    arr = np.load(os.path.join(BATCH_DIR, mf))[\"masks\"]\n    total += arr.shape[0]\n\nprint(\"Total mask samples:\", total)\n\n# ---------- Step 2: Create final HDF5 ----------\nout_path = os.path.join(OUT_DIR, \"sen12ms_masks.h5\")\nh5f = h5py.File(out_path, \"w\")\n\nds = h5f.create_dataset(\n    \"masks\",\n    shape=(total, 64, 64),\n    dtype=\"int16\"\n)\n\n# ---------- Step 3: Stream batches into HDF5 ----------\nwrite_ptr = 0\n\nfor mf in tqdm(mask_files):\n    m = np.load(os.path.join(BATCH_DIR, mf))[\"masks\"]  # (N, 64, 64)\n    n = m.shape[0]\n\n    ds[write_ptr : write_ptr + n] = m\n    write_ptr += n\n\nh5f.close()\n\nprint(\"\\nMask merge completed.\")\nprint(\"Saved to:\", out_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\n\nbands_file = \"/kaggle/working/sen12ms_processed/sen12ms_bands.h5\"\nmasks_file = \"/kaggle/working/sen12ms_processed/sen12ms_masks.h5\"\n\nwith h5py.File(bands_file, \"r\") as f:\n    print(\"BANDS:\")\n    print(\"Keys:\", list(f.keys()))\n    print(\"Shape:\", f[\"bands\"].shape)\n\nwith h5py.File(masks_file, \"r\") as f:\n    print(\"\\nMASKS:\")\n    print(\"Keys:\", list(f.keys()))\n    print(\"Shape:\", f[\"masks\"].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Dataset Merge: EuroSAT + SEN12MS\n\nThis is the final merge step where we combine the pre-processed \n**SEN12MS dataset** (stored in HDF5 format) with the **EuroSAT dataset** (stored in NPY format) \nto create one unified machine-learning‚Äìready dataset.\n\n---\n\n## üìÅ Input Datasets\n\n### **1. SEN12MS (HDF5)**\nLocated in Kaggle input:\n- `sen12ms_bands.h5` ‚Üí `(N1, 64, 64, 4)`\n- `sen12ms_masks.h5` ‚Üí `(N1, 64, 64)`\n- `sen12ms_metadata.json`\n\nThese files are produced after batch processing and merging.\n\n### **2. EuroSAT (NPY)**\nUploaded in Kaggle input:\n- `eurosat_bands.npy` ‚Üí `(N2, 64, 64, 4)`\n- `eurosat_masks.npy` ‚Üí `(N2, 64, 64)`\n\n---\n\n## üì¶ Output (Final Combined Dataset)\n\nAfter merging:\n\n- `final_bands.npy` ‚Üí `(N1 + N2, 64, 64, 4)`\n- `final_masks.npy` ‚Üí `(N1 + N2, 64, 64)`\n- `final_metadata.json`\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport h5py\nimport os\nfrom tqdm import tqdm\n\nOUT = \"/kaggle/working/final_dataset/final_bands.npz\"\nTMP = \"/kaggle/working/final_dataset/_bands_tmp\"\n\n# Create temporary directory to store small chunks\nos.makedirs(TMP, exist_ok=True)\n\n# 1. Load EuroSAT bands (fits in RAM)\neurosat_bands = np.load(\"/kaggle/input/eurosat/eurosat_bands.npy\")\nnp.save(os.path.join(TMP, \"chunk_0.npy\"), eurosat_bands)\n\n# 2. Stream Sen12MS bands in chunks\nwith h5py.File(\"/kaggle/input/sen12ms/sen12ms_bands.h5\") as f:\n    ds = f[\"bands\"]\n    total = ds.shape[0]\n    chunk = 10000  # adjustable\n\n    chunk_id = 1\n\n    for i in tqdm(range(0, total, chunk)):\n        part = ds[i:i+chunk]\n        np.save(os.path.join(TMP, f\"chunk_{chunk_id}.npy\"), part)\n        chunk_id += 1\n\n# 3. Write NPZ with all chunks\nnp.savez_compressed(\n    OUT,\n    **{f\"arr_{i}\": np.load(os.path.join(TMP, f)) \n       for i, f in enumerate(sorted(os.listdir(TMP)))}\n)\n\nprint(\"Final BANDS saved:\", OUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T16:34:45.249484Z","iopub.execute_input":"2025-11-25T16:34:45.249709Z","iopub.status.idle":"2025-11-25T16:58:40.378860Z","shell.execute_reply.started":"2025-11-25T16:34:45.249688Z","shell.execute_reply":"2025-11-25T16:58:40.376923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/final_dataset/_bands_tmp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:14:16.109471Z","iopub.execute_input":"2025-11-25T17:14:16.110847Z","iopub.status.idle":"2025-11-25T17:14:16.841196Z","shell.execute_reply.started":"2025-11-25T17:14:16.110806Z","shell.execute_reply":"2025-11-25T17:14:16.839823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the chunked NPZ\ndata = np.load(\"/kaggle/working/final_dataset/final_bands.npz\")\n\n# Concatenate in correct order\nall_arrays = [data[k] for k in sorted(data.files)]\nmerged = np.concatenate(all_arrays, axis=0)\n\nprint(\"FINAL merged shape:\", merged.shape)\n\n# Save final merged dataset as one clean NPZ\nnp.savez_compressed(\n    \"/kaggle/working/final_dataset/final_bands_merged.npz\",\n    bands=merged\n)\n\nprint(\"Saved FINAL merged dataset as final_bands_merged.npz\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:15:52.493633Z","iopub.execute_input":"2025-11-25T17:15:52.494033Z","iopub.status.idle":"2025-11-25T17:39:22.256588Z","shell.execute_reply.started":"2025-11-25T17:15:52.494004Z","shell.execute_reply":"2025-11-25T17:39:22.254556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndata = np.load(\"/kaggle/working/final_dataset/final_bands_merged.npz\")\n\nfor k in data.files:\n    print(k, data[k].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:49:33.220612Z","iopub.execute_input":"2025-11-25T17:49:33.220974Z","iopub.status.idle":"2025-11-25T17:51:13.906134Z","shell.execute_reply.started":"2025-11-25T17:49:33.220950Z","shell.execute_reply":"2025-11-25T17:51:13.904884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport h5py\nimport os\nfrom tqdm import tqdm\n\nOUT = \"/kaggle/working/final_dataset/final_masks.npz\"\nTMP = \"/kaggle/working/final_dataset/_masks_tmp\"\n\nos.makedirs(TMP, exist_ok=True)\n\n# 1. EuroSAT masks\neurosat_masks = np.load(\"/kaggle/input/eurosat/eurosat_masks.npy\")\nnp.save(os.path.join(TMP, \"chunk_0.npy\"), eurosat_masks)\n\n# 2. Sen12MS masks streamed\nwith h5py.File(\"/kaggle/input/sen12ms/sen12ms_masks.h5\") as f:\n    ds = f[\"masks\"]\n    total = ds.shape[0]\n    chunk = 5000\n\n    chunk_id = 1\n\n    for i in tqdm(range(0, total, chunk)):\n        part = ds[i:i+chunk]\n        np.save(os.path.join(TMP, f\"chunk_{chunk_id}.npy\"), part)\n        chunk_id += 1\n\n# 3. Write NPZ output\nnp.savez_compressed(\n    OUT,\n    **{f\"arr_{i}\": np.load(os.path.join(TMP, f)) \n       for i, f in enumerate(sorted(os.listdir(TMP)))}\n)\n\nprint(\"Final MASKS saved:\", OUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:52:18.991400Z","iopub.execute_input":"2025-11-25T17:52:18.991788Z","iopub.status.idle":"2025-11-25T17:52:40.239203Z","shell.execute_reply.started":"2025-11-25T17:52:18.991761Z","shell.execute_reply":"2025-11-25T17:52:40.238158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/final_dataset/_masks_tmp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the chunked NPZ\ndata = np.load(\"/kaggle/working/final_dataset/final_masks.npz\")\n\n# Concatenate in correct order\nall_arrays = [data[k] for k in sorted(data.files)]\nmerged = np.concatenate(all_arrays, axis=0)\n\nprint(\"FINAL merged shape:\", merged.shape)\n\n# Save final merged dataset as one clean NPZ\nnp.savez_compressed(\n    \"/kaggle/working/final_dataset/final_masks_merged.npz\",\n    masks=merged\n)\n\nprint(\"Saved FINAL merged dataset as final_masks_merged.npz\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:54:07.545897Z","iopub.execute_input":"2025-11-25T17:54:07.546334Z","iopub.status.idle":"2025-11-25T17:54:36.388162Z","shell.execute_reply.started":"2025-11-25T17:54:07.546287Z","shell.execute_reply":"2025-11-25T17:54:36.386708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndata = np.load(\"/kaggle/working/final_dataset/final_masks_merged.npz\")\n\nfor k in data.files:\n    print(k, data[k].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:55:28.733878Z","iopub.execute_input":"2025-11-25T17:55:28.734770Z","iopub.status.idle":"2025-11-25T17:55:36.299084Z","shell.execute_reply.started":"2025-11-25T17:55:28.734733Z","shell.execute_reply":"2025-11-25T17:55:36.297860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:24:55.416112Z","iopub.execute_input":"2025-11-24T18:24:55.416518Z","iopub.status.idle":"2025-11-24T18:25:02.610986Z","shell.execute_reply.started":"2025-11-24T18:24:55.416450Z","shell.execute_reply":"2025-11-24T18:25:02.609591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:55:45.187881Z","iopub.execute_input":"2025-11-25T17:55:45.188241Z","iopub.status.idle":"2025-11-25T17:55:45.802687Z","shell.execute_reply.started":"2025-11-25T17:55:45.188215Z","shell.execute_reply":"2025-11-25T17:55:45.801486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\n\nrepo_id = \"mayeraa/water_body_detection\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:56:06.942685Z","iopub.execute_input":"2025-11-25T17:56:06.943058Z","iopub.status.idle":"2025-11-25T17:56:06.948663Z","shell.execute_reply.started":"2025-11-25T17:56:06.943030Z","shell.execute_reply":"2025-11-25T17:56:06.947273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"api.upload_file(\n    path_or_fileobj=\"/kaggle/working/final_dataset/final_bands_merged.npz\",\n    path_in_repo=\"final_bands_merged.npz\",\n    repo_id=repo_id,\n    repo_type=\"dataset\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:56:28.398835Z","iopub.execute_input":"2025-11-25T17:56:28.399234Z","iopub.status.idle":"2025-11-25T17:57:51.309771Z","shell.execute_reply.started":"2025-11-25T17:56:28.399187Z","shell.execute_reply":"2025-11-25T17:57:51.308617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"api.upload_file(\n    path_or_fileobj=\"/kaggle/working/final_dataset/final_masks_merged.npz\",\n    path_in_repo=\"final_masks_merged.npz\",\n    repo_id=repo_id,\n    repo_type=\"dataset\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T17:58:15.531242Z","iopub.execute_input":"2025-11-25T17:58:15.532560Z","iopub.status.idle":"2025-11-25T17:58:18.592525Z","shell.execute_reply.started":"2025-11-25T17:58:15.532483Z","shell.execute_reply":"2025-11-25T17:58:18.591088Z"}},"outputs":[],"execution_count":null}]}