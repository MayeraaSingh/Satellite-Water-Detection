{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Notebook Overview ‚Äî Satellite Water Detection Model Pipeline\n\nThis notebook walks through the complete end-to-end pipeline for training and evaluating machine-learning models for satellite-based water detection.  \nIt covers everything from loading preprocessed pixel-level data to generating final prediction masks and preparing a handover package.\n\n### üîç What this notebook does:\n\n1. **Load Sample Dataset**  \n   Retrieve the preprocessed feature (`X`) and label (`y`) arrays used for training and testing.\n\n2. **Create Train/Validation/Test Splits**  \n   Split the dataset into 70% training, 15% validation, and 15% testing to ensure fair and consistent evaluation.\n\n3. **Train Baseline Models (XGBoost & LightGBM)**  \n   Train two strong tree-based models, tune key hyperparameters, and evaluate their performance on the validation set.\n\n4. **Select Best Model & Save Artifacts**  \n   Store the trained models and validation metrics for reproducibility and future inference.\n\n5. **Generate Test Predictions & Reshape into 64√ó64 Masks**  \n   Convert pixel-level predictions back into image tiles for visualization and comparison with ground truth.\n\n6. **Prepare a Clean Handover Package**  \n   Organize key outputs (model, metrics, masks, and split details) into a structured folder for easy sharing.\n\n7. **Upload Results to Hugging Face**  \n   The final handover folder is manually uploaded to a Hugging Face dataset repository for accessibility.\n\n---\n\nOverall, this notebook provides a clear, reproducible workflow for building and evaluating satellite image classification models, making it easy for teammates and downstream systems to use the outputs.  \n","metadata":{}},{"cell_type":"markdown","source":"### Step 0 ‚Äî Install and Login to HuggingFace","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub --quiet\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub import login\nlogin()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:18:27.215493Z","iopub.execute_input":"2025-11-26T06:18:27.215844Z","iopub.status.idle":"2025-11-26T06:18:31.391880Z","shell.execute_reply.started":"2025-11-26T06:18:27.215808Z","shell.execute_reply":"2025-11-26T06:18:31.390682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"List All Files in the HuggingFace Dataset Repository","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import list_repo_files\n\nfiles = list_repo_files(\n    repo_id=\"mishhkaa/satellite-water-detection\",\n    repo_type=\"dataset\"\n)\n\nfor f in files:\n    print(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:18:53.900395Z","iopub.execute_input":"2025-11-26T06:18:53.900754Z","iopub.status.idle":"2025-11-26T06:18:54.027761Z","shell.execute_reply.started":"2025-11-26T06:18:53.900719Z","shell.execute_reply":"2025-11-26T06:18:54.026738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 1 ‚Äî Load X_sample and y_sample  \nThis cell downloads the preprocessed feature (`X`) and label (`y`) arrays from the Hugging Face dataset repository.  \nIt then loads them into NumPy arrays and prints their shapes and label distribution to verify successful loading.\n","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 1 ‚Äî Load X_sample and y_sample\n# =========================================================\n\nfrom huggingface_hub import hf_hub_download\nimport numpy as np\n\nrepo = \"mishhkaa/satellite-water-detection\"\n\n# Download and load arrays\nX_path = hf_hub_download(repo_id=repo, filename=\"X_sample.npy\", repo_type=\"dataset\")\ny_path = hf_hub_download(repo_id=repo, filename=\"y_sample.npy\", repo_type=\"dataset\")\n\nX = np.load(X_path)\ny = np.load(y_path)\n\nprint(\"‚úÖ Dataset Loaded Successfully!\")\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\nprint(\"Unique labels in y:\", np.unique(y))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:18:59.992999Z","iopub.execute_input":"2025-11-26T06:18:59.993671Z","iopub.status.idle":"2025-11-26T06:19:02.395902Z","shell.execute_reply.started":"2025-11-26T06:18:59.993641Z","shell.execute_reply":"2025-11-26T06:19:02.394974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2 ‚Äî Train/Val/Test Split  \nThis cell splits the dataset into training, validation, and test sets using `train_test_split`.  \n70% of the data is used for training, while the remaining 30% is evenly divided into validation and test sets.","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 2 ‚Äî Train/Val/Test Split\n# =========================================================\n\nfrom sklearn.model_selection import train_test_split\n\n# First: split train vs temp (validation + test)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.30, random_state=42, shuffle=True\n)\n\n# Next: split validation vs test equally (15% each)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.50, random_state=42, shuffle=True\n)\n\nprint(\"Split Completed!\")\nprint(\"Train:\", X_train.shape, y_train.shape)\nprint(\"Val:\", X_val.shape, y_val.shape)\nprint(\"Test:\", X_test.shape, y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:19:08.326546Z","iopub.execute_input":"2025-11-26T06:19:08.326856Z","iopub.status.idle":"2025-11-26T06:19:09.123914Z","shell.execute_reply.started":"2025-11-26T06:19:08.326833Z","shell.execute_reply":"2025-11-26T06:19:09.122846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3 ‚Äî Train Baseline XGBoost Model  \nThis cell installs XGBoost, defines a baseline `XGBClassifier` with tuned hyperparameters, and trains it on the training split.  \nAfter training, it evaluates the model on the validation set using Accuracy and F1 Score to measure baseline performance.\n","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 3 ‚Äî Train Baseline XGBoost Model\n# =========================================================\n\n!pip install xgboost --quiet\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Define model\nxgb_model = XGBClassifier(\n    n_estimators=200,\n    max_depth=8,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    tree_method=\"hist\",\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training XGBoost model...\")\nxgb_model.fit(X_train, y_train)\n\n# Predict on validation set\nval_preds = xgb_model.predict(X_val)\n\n# Compute accuracy & F1\nval_acc = accuracy_score(y_val, val_preds)\nval_f1 = f1_score(y_val, val_preds)\n\nprint(\"üèÅ Training Complete!\")\nprint(\"Validation Accuracy:\", val_acc)\nprint(\"Validation F1 Score:\", val_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:22:37.777941Z","iopub.execute_input":"2025-11-26T06:22:37.778324Z","iopub.status.idle":"2025-11-26T06:23:15.004446Z","shell.execute_reply.started":"2025-11-26T06:22:37.778290Z","shell.execute_reply":"2025-11-26T06:23:15.002985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step ‚Äî Train LightGBM Model  \nThis cell sets up LightGBM datasets, trains a Gradient Boosting model with early stopping, and logs progress every 10 rounds.  \nAfter training, it predicts on the validation set and computes Accuracy and F1 Score to evaluate performance.\n","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 4 ‚Äî LightGBM Training \n# =========================================================\n\n!pip install lightgbm --quiet\n\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, f1_score\n\nprint(\"üèÅ Preparing LightGBM datasets...\")\n\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_val = lgb.Dataset(X_val, label=y_val)\n\nprint(\"üöÄ Starting LightGBM Training...\")\n\nlgb_model = lgb.train(\n    params={\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"boosting_type\": \"gbdt\",\n        \"num_leaves\": 64,\n        \"learning_rate\": 0.05,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"max_depth\": -1,\n        \"verbose\": -1\n    },\n    train_set=lgb_train,\n    valid_sets=[lgb_train, lgb_val],\n    num_boost_round=200,\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=30),\n        lgb.log_evaluation(period=10)   # Print every 10 rounds\n    ]\n)\n\nprint(\"üéâ Training finished!\")\n\n# Make predictions\nval_preds_lgb = (lgb_model.predict(X_val) > 0.5).astype(int)\n\n# Metrics\nval_acc_lgb = accuracy_score(y_val, val_preds_lgb)\nval_f1_lgb = f1_score(y_val, val_preds_lgb)\n\nprint(\"\\nüìà LightGBM Validation Results\")\nprint(\"Validation Accuracy:\", val_acc_lgb)\nprint(\"Validation F1 Score:\", val_f1_lgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:20:30.291810Z","iopub.execute_input":"2025-11-26T06:20:30.292178Z","iopub.status.idle":"2025-11-26T06:21:44.831908Z","shell.execute_reply.started":"2025-11-26T06:20:30.292147Z","shell.execute_reply":"2025-11-26T06:21:44.830811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 5 ‚Äî Save Models and Metrics  \nThis cell saves the trained XGBoost and LightGBM models using `joblib`, and writes all validation metrics into a JSON file.  \nIt also creates `models/` and `results/` directories to keep saved artifacts organized.\n","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 5 ‚Äî Save Models & Metrics\n# =========================================================\n\nimport joblib\nimport json\nimport os\n\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\n\n# Save models\njoblib.dump(xgb_model, \"models/xgb_model.pkl\")\njoblib.dump(lgb_model, \"models/lgbm_model.pkl\")\njoblib.dump(lgb_model, \"models/best_model.pkl\")\n\n# Save validation results\nmetrics = {\n    \"xgboost_accuracy\": float(val_acc),\n    \"xgboost_f1\": float(val_f1),\n    \"lightgbm_accuracy\": float(val_acc_lgb),\n    \"lightgbm_f1\": float(val_f1_lgb),\n    \"best_model\": \"lightgbm\"\n}\n\nwith open(\"results/metrics.json\", \"w\") as f:\n    json.dump(metrics, f, indent=4)\n\nprint(\"‚úÖ Models and metrics saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:26:02.784674Z","iopub.execute_input":"2025-11-26T06:26:02.784999Z","iopub.status.idle":"2025-11-26T06:26:02.876027Z","shell.execute_reply.started":"2025-11-26T06:26:02.784977Z","shell.execute_reply":"2025-11-26T06:26:02.875107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 6 ‚Äî Generate Test Predictions & Reshape into Masks  \nThis cell uses the trained LightGBM model to predict labels for the test set, trims the predictions to a multiple of 4096 pixels, and reshapes them into 64√ó64 mask tiles.  \nBoth predicted and true masks are then saved for later visualization or evaluation.","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 6 ‚Äî Generate Predicted Masks for Test Set\n# =========================================================\n\nimport numpy as np\n\nprint(\"üìå Predicting on test set using best LightGBM model...\")\n\ntest_preds = (lgb_model.predict(X_test) > 0.5).astype(int)\n\nprint(\"Raw prediction shape:\", test_preds.shape)\n\n# Ensure divisible by 4096 pixels\nusable_pixels = (test_preds.shape[0] // 4096) * 4096\n\ntest_preds = test_preds[:usable_pixels]\ny_test_trimmed = y_test[:usable_pixels]\n\nnum_tiles = usable_pixels // 4096\n\n# Reshape predictions & ground truth back to tile format\npredicted_masks = test_preds.reshape(num_tiles, 64, 64)\ntrue_masks = y_test_trimmed.reshape(num_tiles, 64, 64)\n\nprint(\"Predicted masks shape:\", predicted_masks.shape)\nprint(\"True masks shape:\", true_masks.shape)\n\n# Save arrays\nos.makedirs(\"predictions\", exist_ok=True)\nnp.save(\"predictions/predicted_masks.npy\", predicted_masks)\nnp.save(\"predictions/true_masks.npy\", true_masks)\n\nprint(\"üéâ Predicted masks saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:28:22.182956Z","iopub.execute_input":"2025-11-26T06:28:22.183443Z","iopub.status.idle":"2025-11-26T06:28:27.417010Z","shell.execute_reply.started":"2025-11-26T06:28:22.183412Z","shell.execute_reply":"2025-11-26T06:28:27.415760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 7 ‚Äî Prepare Handover Package  \nThis cell records dataset split details into a JSON file and copies key artifacts‚Äîincluding metrics, the best model, and prediction masks‚Äîinto a dedicated `handover` folder.  \nIt ensures all essential files are neatly packaged for delivery or further processing.\n","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# STEP 7 ‚Äî Package deliverables\n# =========================================================\n\nimport json\nimport os\n\nos.makedirs(\"handover\", exist_ok=True)\n\n# Save split information\nsplit_info = {\n    \"train_size\": len(X_train),\n    \"val_size\": len(X_val),\n    \"test_size\": len(X_test),\n    \"tile_size\": [64, 64],\n    \"num_test_tiles\": predicted_masks.shape[0]\n}\n\nwith open(\"handover/train_test_split_info.json\", \"w\") as f:\n    json.dump(split_info, f, indent=4)\n\n# Move files into handover folder\n!cp results/metrics.json handover/\n!cp models/best_model.pkl handover/\n!cp predictions/predicted_masks.npy handover/\n!cp predictions/true_masks.npy handover/\n\nprint(\"üéÅ Handover package prepared successfully!\")\nprint(\"Files in handover folder:\")\n!ls handover\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:29:51.143903Z","iopub.execute_input":"2025-11-26T06:29:51.144728Z","iopub.status.idle":"2025-11-26T06:29:51.825654Z","shell.execute_reply.started":"2025-11-26T06:29:51.144695Z","shell.execute_reply":"2025-11-26T06:29:51.824438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# MOVE ALL MODEL FILES INTO HANDOVER\n# =========================================================\n\nimport os\n\n# Ensure folder exists\nos.makedirs(\"handover\", exist_ok=True)\n\n# Copy both models using shell command\n!cp models/xgb_model.pkl handover/\n!cp models/lgbm_model.pkl handover/\n\n# Show folder content\nprint(\"üì¶ Updated handover folder now contains:\")\n!ls -lh handover\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-26T15:50:46.268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Step ‚Äî Upload Handover Package to Hugging Face  \nThe generated `handover` folder (containing the best model, metrics, and prediction masks) was manually uploaded to a Hugging Face dataset repository.  \nThis allows easy sharing, versioning, and access for downstream tasks or team members.","metadata":{}}]}